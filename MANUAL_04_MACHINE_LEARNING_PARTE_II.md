# Manual de Machine Learning - Sistema HidrÃ¡ulico Industrial - Parte II

## ğŸ“‹ ContinuaÃ§Ã£o - AnÃ¡lise Espectral e Temporal

---

## ğŸµ Features Espectrais

### AnÃ¡lise de DomÃ­nio da FrequÃªncia

As features espectrais capturam caracterÃ­sticas no **domÃ­nio da frequÃªncia**, essenciais para detectar padrÃµes oscilatÃ³rios e componentes harmÃ´nicas indicativas de problemas operacionais.

#### ğŸ”¬ ImplementaÃ§Ã£o da AnÃ¡lise Espectral

```python
def _extract_spectral_features(self, features, feature_names, exp_pressure, rec_pressure):
    """
    Extrai features espectrais avanÃ§adas usando FFT
    
    Para pressÃ£o expedidor e recebedor:
    1. Energia em baixa frequÃªncia (0 - 10% Nyquist)
    2. Energia em mÃ©dia frequÃªncia (10% - 30% Nyquist)  
    3. Energia em alta frequÃªncia (30% - 100% Nyquist)
    4. FrequÃªncia dominante
    
    Total: 2 sinais Ã— 4 features = 8 features espectrais
    """
    
    for signal, name in [(exp_pressure, 'exp_p'), (rec_pressure, 'rec_p')]:
        # FFT do sinal
        fft_signal = np.fft.fft(signal)
        freqs = np.fft.fftfreq(len(signal))
        magnitude = np.abs(fft_signal)
        
        # Define bandas de frequÃªncia
        low_freq_mask = np.abs(freqs) < 0.1      # 0-10% da frequÃªncia de Nyquist
        mid_freq_mask = (np.abs(freqs) >= 0.1) & (np.abs(freqs) < 0.3)  # 10-30%
        high_freq_mask = np.abs(freqs) >= 0.3    # 30-100%
        
        # Energia total para normalizaÃ§Ã£o
        total_energy = np.sum(magnitude)
        
        # Energias por banda (normalizadas)
        low_energy = np.sum(magnitude[low_freq_mask]) / max(total_energy, 1e-10)
        mid_energy = np.sum(magnitude[mid_freq_mask]) / max(total_energy, 1e-10)
        high_energy = np.sum(magnitude[high_freq_mask]) / max(total_energy, 1e-10)
        
        # FrequÃªncia dominante (apenas frequÃªncias positivas)
        positive_freqs = freqs[:len(freqs)//2]
        positive_magnitude = magnitude[:len(magnitude)//2]
        dominant_freq_idx = np.argmax(positive_magnitude)
        dominant_freq = positive_freqs[dominant_freq_idx]
        
        features.extend([low_energy, mid_energy, high_energy, dominant_freq])
        feature_names.extend([
            f'{name}_low_energy', f'{name}_mid_energy', f'{name}_high_energy', f'{name}_dom_freq'
        ])
```

#### ğŸ“Š InterpretaÃ§Ã£o das Bandas Espectrais

##### **Energia em Baixa FrequÃªncia (0-10% Nyquist)**

```python
def interpret_low_frequency_energy(self, low_energy_ratio):
    """
    Interpreta energia em baixa frequÃªncia
    
    CaracterÃ­sticas:
    - TendÃªncias de longo prazo
    - Componente DC
    - VariaÃ§Ãµes lentas de pressÃ£o
    """
    
    if low_energy_ratio > 0.8:
        return {
            'interpretation': 'DominÃ¢ncia de componentes lentas',
            'indication': 'Sistema estÃ¡vel, possÃ­vel drift lento',
            'action': 'Monitorar tendÃªncias de longo prazo'
        }
    elif low_energy_ratio < 0.3:
        return {
            'interpretation': 'Baixa energia em componentes lentas',
            'indication': 'PossÃ­vel instabilidade ou ruÃ­do dominante',
            'action': 'Investigar fontes de ruÃ­do de alta frequÃªncia'
        }
    else:
        return {
            'interpretation': 'DistribuiÃ§Ã£o espectral equilibrada',
            'indication': 'Sistema com comportamento normal',
            'action': 'Monitoramento rotineiro'
        }
```

##### **Energia em Alta FrequÃªncia (30-100% Nyquist)**

```python
def interpret_high_frequency_energy(self, high_energy_ratio):
    """
    Interpreta energia em alta frequÃªncia
    
    CaracterÃ­sticas:
    - RuÃ­do de instrumentaÃ§Ã£o
    - TurbulÃªncia local
    - VibraÃ§Ãµes mecÃ¢nicas
    - PossÃ­veis vazamentos (jato turbulento)
    """
    
    if high_energy_ratio > 0.3:
        return {
            'interpretation': 'Alta energia em componentes rÃ¡pidas',
            'indication': 'PossÃ­vel turbulÃªncia, vazamento ou ruÃ­do',
            'severity': 'Alto',
            'action': 'InvestigaÃ§Ã£o imediata recomendada'
        }
    elif high_energy_ratio > 0.15:
        return {
            'interpretation': 'Energia moderada em alta frequÃªncia',
            'indication': 'PossÃ­vel inÃ­cio de instabilidade',
            'severity': 'Moderado',
            'action': 'Monitoramento intensificado'
        }
    else:
        return {
            'interpretation': 'Baixa energia em alta frequÃªncia',
            'indication': 'Sistema estÃ¡vel, ruÃ­do controlado',
            'severity': 'Baixo',
            'action': 'Monitoramento normal'
        }
```

#### ğŸ¼ AnÃ¡lise de HarmÃ´nicos

```python
def harmonic_analysis(self, signal, fundamental_freq=None):
    """
    AnÃ¡lise de conteÃºdo harmÃ´nico para detecÃ§Ã£o de padrÃµes periÃ³dicos
    
    Detecta:
    - FrequÃªncia fundamental
    - HarmÃ´nicos (2f, 3f, 4f, ...)
    - DistorÃ§Ã£o harmÃ´nica total (THD)
    - Componentes inter-harmÃ´nicas
    """
    
    # FFT do sinal
    fft_signal = np.fft.fft(signal)
    freqs = np.fft.fftfreq(len(signal))
    magnitude = np.abs(fft_signal)
    
    # Encontra frequÃªncia fundamental se nÃ£o fornecida
    if fundamental_freq is None:
        # Pico de maior magnitude (excluindo DC)
        positive_freqs = freqs[1:len(freqs)//2]  # Exclui DC e negativos
        positive_magnitude = magnitude[1:len(magnitude)//2]
        fundamental_idx = np.argmax(positive_magnitude)
        fundamental_freq = positive_freqs[fundamental_idx]
    
    # Localiza harmÃ´nicos
    harmonics = {}
    total_power = np.sum(magnitude**2)
    fundamental_power = 0
    
    for n in range(1, 6):  # AtÃ© 5Âº harmÃ´nico
        harmonic_freq = n * fundamental_freq
        
        # Encontra bin mais prÃ³ximo da frequÃªncia harmÃ´nica
        harmonic_idx = np.argmin(np.abs(freqs - harmonic_freq))
        harmonic_magnitude = magnitude[harmonic_idx]
        harmonic_power = harmonic_magnitude**2
        
        if n == 1:
            fundamental_power = harmonic_power
        
        harmonics[f'harmonic_{n}'] = {
            'frequency': freqs[harmonic_idx],
            'magnitude': harmonic_magnitude,
            'power_ratio': harmonic_power / max(total_power, 1e-10)
        }
    
    # DistorÃ§Ã£o HarmÃ´nica Total (THD)
    harmonic_powers = sum([h['magnitude']**2 for n, h in harmonics.items() if n != 'harmonic_1'])
    thd = np.sqrt(harmonic_powers / max(fundamental_power, 1e-10)) * 100  # Percentual
    
    return {
        'fundamental_frequency': fundamental_freq,
        'harmonics': harmonics,
        'thd_percent': thd,
        'spectral_quality': 'clean' if thd < 5 else 'distorted' if thd < 15 else 'heavily_distorted'
    }
```

---

## â±ï¸ Features de Estabilidade Temporal

### AnÃ¡lise de PersistÃªncia e FlutuaÃ§Ãµes

As features temporais quantificam a **estabilidade** dos sinais ao longo do tempo, usando janelas mÃ³veis para detectar mudanÃ§as graduais.

#### ğŸ“ˆ ImplementaÃ§Ã£o da AnÃ¡lise Temporal

```python
def _extract_temporal_features(self, features, feature_names, exp_pressure, rec_pressure, flow):
    """
    Extrai features de estabilidade temporal usando janelas mÃ³veis
    
    Para pressÃ£o expedidor, pressÃ£o recebedor e fluxo:
    1. Estabilidade mÃ©dia em janela mÃ³vel
    2. MÃ¡ximo coeficiente de variaÃ§Ã£o local
    
    Total: 3 sinais Ã— 2 features = 6 features temporais
    """
    
    for signal, name in [(exp_pressure, 'exp_p'), (rec_pressure, 'rec_p'), (flow, 'flow')]:
        # Janela mÃ³vel adaptativa
        window_size = min(10, len(signal)//4)
        
        if window_size > 1:
            stabilities = []
            cv_locals = []  # Coeficientes de variaÃ§Ã£o locais
            
            # AnÃ¡lise em janelas deslizantes
            for i in range(len(signal) - window_size + 1):
                window_data = signal[i:i + window_size]
                
                # Estabilidade = 1 / (1 + coeficiente de variaÃ§Ã£o)
                mean_window = np.mean(window_data)
                std_window = np.std(window_data)
                cv = std_window / max(abs(mean_window), 1e-6)  # Coef. variaÃ§Ã£o
                
                stability = 1.0 / (1.0 + cv)
                stabilities.append(stability)
                cv_locals.append(cv)
            
            stability_mean = np.mean(stabilities)
            cv_max = np.max(cv_locals)
        else:
            # Dados insuficientes para janela mÃ³vel
            stability_mean = 1.0  # Assumir estÃ¡vel
            cv_max = 0.0
        
        features.extend([stability_mean, cv_max])
        feature_names.extend([f'{name}_stability', f'{name}_cv_max'])
```

#### ğŸ¯ MÃ©tricas de Estabilidade

##### **Coeficiente de VariaÃ§Ã£o Local**

```
CV = Ïƒ_window / |Î¼_window|
```

- **InterpretaÃ§Ã£o**: Variabilidade relativa em cada janela
- **AplicaÃ§Ã£o**: Detecta regiÃµes de instabilidade localizada
- **Vazamentos**: CV elevado indica flutuaÃ§Ãµes caracterÃ­sticas

##### **Ãndice de Estabilidade**

```
Stability = 1 / (1 + CV)
```

- **Valores**: 0 (instÃ¡vel) a 1 (perfeitamente estÃ¡vel)
- **InterpretaÃ§Ã£o**: ResistÃªncia a flutuaÃ§Ãµes
- **AplicaÃ§Ã£o**: Quantifica qualidade operacional

#### ğŸ” AnÃ¡lise de MudanÃ§as de Regime

```python
def detect_regime_changes(self, signal, min_regime_length=20):
    """
    Detecta mudanÃ§as de regime operacional usando CUSUM e anÃ¡lise de variÃ¢ncia
    
    Identifica:
    - Pontos de mudanÃ§a na mÃ©dia
    - Pontos de mudanÃ§a na variÃ¢ncia  
    - DuraÃ§Ã£o de cada regime
    - CaracterizaÃ§Ã£o dos regimes
    """
    
    # CUSUM para detecÃ§Ã£o de mudanÃ§a na mÃ©dia
    mean_baseline = np.mean(signal)
    cusum_pos = np.zeros(len(signal))
    cusum_neg = np.zeros(len(signal))
    
    drift = 0.01 * np.std(signal)  # Drift mÃ­nimo para detecÃ§Ã£o
    threshold = 3 * np.std(signal)  # Threshold de detecÃ§Ã£o
    
    change_points_mean = []
    
    for i in range(1, len(signal)):
        # CUSUM acumulativo
        cusum_pos[i] = max(0, cusum_pos[i-1] + signal[i] - mean_baseline - drift)
        cusum_neg[i] = max(0, cusum_neg[i-1] - signal[i] + mean_baseline - drift)
        
        # DetecÃ§Ã£o de mudanÃ§a
        if cusum_pos[i] > threshold or cusum_neg[i] > threshold:
            change_points_mean.append(i)
            # Reset CUSUM
            cusum_pos[i] = 0
            cusum_neg[i] = 0
    
    # AnÃ¡lise de mudanÃ§a na variÃ¢ncia usando F-test em janelas
    change_points_variance = []
    window_size = min(50, len(signal)//5)
    
    for i in range(window_size, len(signal) - window_size):
        # Janelas antes e depois do ponto candidato
        before_window = signal[i-window_size:i]
        after_window = signal[i:i+window_size]
        
        # F-test para igualdade de variÃ¢ncias
        var_before = np.var(before_window)
        var_after = np.var(after_window)
        
        f_ratio = max(var_before, var_after) / max(min(var_before, var_after), 1e-10)
        
        # Threshold empÃ­rico para F-test
        if f_ratio > 2.5:  # MudanÃ§a significativa na variÃ¢ncia
            change_points_variance.append(i)
    
    # Consolida pontos de mudanÃ§a
    all_changes = sorted(set(change_points_mean + change_points_variance))
    
    # Remove pontos muito prÃ³ximos
    filtered_changes = []
    for change in all_changes:
        if not filtered_changes or change - filtered_changes[-1] > min_regime_length:
            filtered_changes.append(change)
    
    # Caracteriza cada regime
    regimes = []
    start_points = [0] + filtered_changes
    end_points = filtered_changes + [len(signal)]
    
    for i, (start, end) in enumerate(zip(start_points, end_points)):
        regime_data = signal[start:end]
        
        regimes.append({
            'regime_id': i + 1,
            'start_index': start,
            'end_index': end,
            'duration': end - start,
            'mean_level': np.mean(regime_data),
            'std_level': np.std(regime_data),
            'trend': np.polyfit(range(len(regime_data)), regime_data, 1)[0] if len(regime_data) > 1 else 0.0
        })
    
    return {
        'n_regimes': len(regimes),
        'change_points': filtered_changes,
        'regimes': regimes,
        'stability_score': 1.0 / max(1, len(filtered_changes))  # Mais mudanÃ§as = menos estÃ¡vel
    }
```

---

## ğŸ”— RelaÃ§Ãµes Entre VariÃ¡veis

### CorrelaÃ§Ãµes Cruzadas MultivariÃ¡veis

As features relacionais capturam **interaÃ§Ãµes entre diferentes variÃ¡veis**, essenciais para compreender o comportamento sistÃªmico do processo hidrÃ¡ulico.

#### ğŸ¯ ImplementaÃ§Ã£o das CorrelaÃ§Ãµes Cruzadas

```python
def _extract_relational_features(self, features, feature_names, exp_pressure, rec_pressure, flow, density, temperature):
    """
    Extrai features baseadas em correlaÃ§Ãµes entre pares de variÃ¡veis
    
    Pares analisados:
    1. (DiferenÃ§a de pressÃ£o) vs (Fluxo)
    2. (Densidade) vs (Temperatura)  
    3. (Fluxo) vs (Temperatura)
    4. (DiferenÃ§a de pressÃ£o) vs (Densidade)
    
    Total: 4 pares Ã— 2 features = 8 features relacionais
    """
    
    # Calcula diferenÃ§a de pressÃ£o (indicador de forÃ§a motriz)
    pressure_diff = exp_pressure - rec_pressure
    
    # Define pares de variÃ¡veis para anÃ¡lise
    variable_pairs = [
        (pressure_diff, flow, 'press_diff_flow'),      # Lei de Darcy-Weisbach
        (density, temperature, 'density_temp'),        # RelaÃ§Ã£o termodinÃ¢mica
        (flow, temperature, 'flow_temp'),              # Efeito tÃ©rmico no fluxo
        (pressure_diff, density, 'press_diff_density') # Efeito baromÃ©trico
    ]
    
    correlations = []
    correlation_names = []
    
    for var1, var2, name in variable_pairs:
        if len(var1) > 1 and len(var2) > 1:
            # CorrelaÃ§Ã£o de Pearson
            try:
                corr_matrix = np.corrcoef(var1, var2)
                correlation = corr_matrix[0, 1]
                
                # Trata NaN (variÃ¡veis constantes)
                if np.isnan(correlation):
                    correlation = 0.0
                
            except:
                correlation = 0.0
            
            # CorrelaÃ§Ã£o cruzada com delay (correlaÃ§Ã£o mÃ¡xima com lag)
            try:
                cross_corr = signal.correlate(var1 - np.mean(var1), var2 - np.mean(var2), mode='full')
                cross_corr_normalized = cross_corr / (len(var1) * np.std(var1) * np.std(var2))
                max_cross_corr = np.max(np.abs(cross_corr_normalized))
                
                if np.isnan(max_cross_corr):
                    max_cross_corr = 0.0
                    
            except:
                max_cross_corr = 0.0
        else:
            correlation = 0.0
            max_cross_corr = 0.0
        
        correlations.extend([correlation, max_cross_corr])
        correlation_names.extend([f'corr_{name}', f'xcorr_max_{name}'])
    
    features.extend(correlations)
    feature_names.extend(correlation_names)
```

#### ğŸ“Š InterpretaÃ§Ã£o FÃ­sica das CorrelaÃ§Ãµes

##### **DiferenÃ§a de PressÃ£o vs Fluxo**

```python
def interpret_pressure_flow_correlation(self, correlation):
    """
    Interpreta correlaÃ§Ã£o entre diferenÃ§a de pressÃ£o e fluxo
    
    Baseado na EquaÃ§Ã£o de Darcy-Weisbach:
    Î”P = f Ã— (L/D) Ã— (ÏVÂ²/2)
    
    Onde Î”P âˆ QÂ² (fluxo ao quadrado)
    """
    
    if correlation > 0.8:
        return {
            'interpretation': 'CorrelaÃ§Ã£o forte e positiva',
            'physical_meaning': 'Comportamento hidrÃ¡ulico normal',
            'flow_regime': 'Turbulento bem estabelecido',
            'system_health': 'Bom'
        }
    elif correlation > 0.5:
        return {
            'interpretation': 'CorrelaÃ§Ã£o moderada',
            'physical_meaning': 'PossÃ­vel transiÃ§Ã£o de regime ou perdas adicionais',
            'flow_regime': 'TransiÃ§Ã£o laminar-turbulento',
            'system_health': 'AtenÃ§Ã£o'
        }
    elif correlation < 0.3:
        return {
            'interpretation': 'CorrelaÃ§Ã£o fraca ou ausente',
            'physical_meaning': 'PossÃ­vel vazamento, bloqueio ou instrumentaÃ§Ã£o defeituosa',
            'flow_regime': 'Indeterminado',
            'system_health': 'CrÃ­tico'
        }
```

##### **Densidade vs Temperatura**

```python
def interpret_density_temperature_correlation(self, correlation):
    """
    Interpreta correlaÃ§Ã£o entre densidade e temperatura
    
    Baseado na equaÃ§Ã£o de estado:
    Ï = Ïâ‚€ Ã— [1 - Î²(T - Tâ‚€)]
    
    Onde Î² Ã© o coeficiente de expansÃ£o tÃ©rmica
    """
    
    expected_correlation = -0.7  # CorrelaÃ§Ã£o negativa esperada
    
    if correlation < -0.6:
        return {
            'interpretation': 'CorrelaÃ§Ã£o negativa forte (esperada)',
            'physical_meaning': 'ExpansÃ£o tÃ©rmica normal do fluido',
            'fluid_behavior': 'Conforme esperado',
            'measurement_quality': 'Boa'
        }
    elif correlation > -0.3:
        return {
            'interpretation': 'CorrelaÃ§Ã£o negativa fraca ou positiva',
            'physical_meaning': 'PossÃ­vel mistura de fluidos, mudanÃ§a de composiÃ§Ã£o ou erro de instrumentaÃ§Ã£o',
            'fluid_behavior': 'AnÃ´malo',
            'measurement_quality': 'QuestionÃ¡vel'
        }
```

#### ğŸ” AnÃ¡lise de Causalidade Temporal

```python
def granger_causality_analysis(self, var1, var2, max_lag=10):
    """
    Teste de Causalidade de Granger entre duas variÃ¡veis
    
    Determina se var1 "causa" var2 no sentido estatÃ­stico:
    - var1 ajuda a predizer var2 melhor que apenas o histÃ³rico de var2
    """
    
    from statsmodels.tsa.stattools import grangercausalitytests
    
    try:
        # Prepara dados para teste
        data = np.column_stack([var2, var1])  # [y, x] - ordem importante
        
        # Executa teste de Granger
        results = grangercausalitytests(data, max_lag, verbose=False)
        
        # Extrai p-valores para diferentes lags
        p_values = {}
        f_statistics = {}
        
        for lag in range(1, max_lag + 1):
            if lag in results:
                test_result = results[lag][0]['ssr_ftest']
                p_values[lag] = test_result[1]  # p-valor
                f_statistics[lag] = test_result[0]  # estatÃ­stica F
        
        # Determina o melhor lag (menor p-valor)
        best_lag = min(p_values.keys(), key=lambda k: p_values[k])
        best_p_value = p_values[best_lag]
        
        # InterpretaÃ§Ã£o
        significance_level = 0.05
        is_causal = best_p_value < significance_level
        
        return {
            'is_causal': is_causal,
            'best_lag': best_lag,
            'p_value': best_p_value,
            'f_statistic': f_statistics[best_lag],
            'all_lags': {
                'p_values': p_values,
                'f_statistics': f_statistics
            },
            'interpretation': 'var1 Granger-causa var2' if is_causal else 'Sem causalidade detectada'
        }
        
    except Exception as e:
        return {
            'error': f'Erro no teste de Granger: {str(e)}',
            'is_causal': False,
            'p_value': 1.0
        }
```

---

## ğŸª IntegraÃ§Ã£o de Features

### Vetor de Features Completo

Ao final do processo de extraÃ§Ã£o, o sistema produz um **vetor de 81 features** altamente especializadas:

#### ğŸ“‹ Resumo das Features por Categoria

| Categoria | Quantidade | DescriÃ§Ã£o | AplicaÃ§Ã£o Principal |
|-----------|------------|-----------|-------------------|
| **EstatÃ­sticas** | 45 | Momentos, percentis, tendÃªncias | CaracterizaÃ§Ã£o bÃ¡sica |
| **Gradientes** | 9 | Derivadas temporais | DetecÃ§Ã£o de transientes |
| **CorrelaÃ§Ã£o Cruzada** | 5 | AnÃ¡lise sÃ´nica integrada | PropagaÃ§Ã£o de ondas |
| **Espectrais** | 8 | ConteÃºdo de frequÃªncias | PadrÃµes oscilatÃ³rios |
| **Relacionais** | 8 | CorrelaÃ§Ãµes entre variÃ¡veis | Comportamento sistÃªmico |
| **Temporais** | 6 | Estabilidade e persistÃªncia | MudanÃ§as de regime |
| **TOTAL** | **81** | **Features altamente especializadas** | **DetecÃ§Ã£o completa** |

#### ğŸ§® Vetor de Features Normalizado

```python
def prepare_feature_vector_for_ml(self, features_raw):
    """
    Prepara vetor de features para algoritmos de ML
    
    Processos:
    1. NormalizaÃ§Ã£o Z-score
    2. Tratamento de outliers
    3. Preenchimento de valores faltantes
    4. ValidaÃ§Ã£o de consistÃªncia
    """
    
    # Remove NaN e infinitos
    features_clean = np.array(features_raw)
    nan_mask = np.isnan(features_clean) | np.isinf(features_clean)
    features_clean[nan_mask] = 0.0  # Ou mediana/interpolaÃ§Ã£o
    
    # DetecÃ§Ã£o de outliers extremos (> 5 sigma)
    outlier_mask = np.abs(features_clean) > 5 * np.std(features_clean)
    features_clean[outlier_mask] = np.clip(
        features_clean[outlier_mask],
        -5 * np.std(features_clean),
        5 * np.std(features_clean)
    )
    
    # NormalizaÃ§Ã£o usando scaler treinado
    if hasattr(self.scaler, 'scale_'):  # Scaler jÃ¡ foi treinado
        features_normalized = self.scaler.transform(features_clean.reshape(1, -1))[0]
    else:
        # Primeira vez - fit e transform
        features_normalized = features_clean  # Retorna sem normalizar
    
    # ValidaÃ§Ã£o final
    assert len(features_normalized) == 81, f"Esperadas 81 features, obtidas {len(features_normalized)}"
    assert not np.any(np.isnan(features_normalized)), "Features contÃªm NaN apÃ³s normalizaÃ§Ã£o"
    
    return features_normalized
```

---

**CONTINUAÃ‡ÃƒO NA PARTE III**

A Parte II cobriu detalhadamente:

- âœ… **Features Espectrais** - AnÃ¡lise FFT, bandas de frequÃªncia, harmÃ´nicos
- âœ… **Features Temporais** - Estabilidade, janelas mÃ³veis, mudanÃ§as de regime  
- âœ… **Features Relacionais** - CorrelaÃ§Ãµes cruzadas, causalidade de Granger
- âœ… **IntegraÃ§Ã£o de Features** - Vetor final de 81 features

**PrÃ³xima parte** - MANUAL_04_MACHINE_LEARNING_PARTE_III.md:

- ğŸ”„ **Algoritmos ML** - Isolation Forest, Random Forest, SVM, DBSCAN
- ğŸ§  **Treinamento Adaptativo** - Retreino automÃ¡tico, threshold dinÃ¢mico
- ğŸ“Š **AnÃ¡lise PCA** - ReduÃ§Ã£o de dimensionalidade, componentes principais
- ğŸ¯ **PrediÃ§Ã£o e FusÃ£o** - CombinaÃ§Ã£o de modelos, confidence scoring

Continuar com a Parte III?
