# Manual de Machine Learning - Sistema Hidr√°ulico Industrial - Parte I

## üìã √çndice do Machine Learning

### **PARTE I - Fundamentos e Extra√ß√£o de Features**

1. [Vis√£o Geral do Sistema ML](#vis√£o-geral-do-sistema-ml)
2. [Arquitetura Adaptativa](#arquitetura-adaptativa)
3. [Extra√ß√£o de Features Avan√ßadas](#extra√ß√£o-de-features-avan√ßadas)
4. [Features Estat√≠sticas](#features-estat√≠sticas)
5. [Features de Gradiente](#features-de-gradiente)
6. [Features de Correla√ß√£o Cruzada](#features-de-correla√ß√£o-cruzada)

### **PARTE II - An√°lise Espectral e Temporal**

1. [Features Espectrais](#features-espectrais)
2. [An√°lise de Frequ√™ncias](#an√°lise-de-frequ√™ncias)
3. [Features de Estabilidade Temporal](#features-de-estabilidade-temporal)
4. [Rela√ß√µes Entre Vari√°veis](#rela√ß√µes-entre-vari√°veis)

### **PARTE III - Algoritmos de Machine Learning**

1. [Isolation Forest](#isolation-forest)
2. [Random Forest](#random-forest)
3. [Support Vector Machines](#support-vector-machines)
4. [DBSCAN Clustering](#dbscan-clustering)
5. [An√°lise PCA](#an√°lise-pca)
6. [Correla√ß√£o Can√¥nica](#correla√ß√£o-can√¥nica)

---

## üß† Vis√£o Geral do Sistema ML

### Classe AdaptiveMLSystem

O sistema de Machine Learning √© implementado atrav√©s da classe `AdaptiveMLSystem`, que representa um **sistema adaptativo de aprendizado de m√°quina** especializado em an√°lise hidr√°ulica industrial.

#### üèóÔ∏è Arquitetura Principal

```python
class AdaptiveMLSystem:
    """
    Sistema de Machine Learning que aprende com vazamentos confirmados
    
    Caracter√≠sticas:
    - Aprendizado incremental adaptativo
    - M√∫ltiplos algoritmos especializados
    - Cache inteligente de features
    - Retreino autom√°tico baseado em threshold
    """
    
    def __init__(self, system_config: SystemConfiguration):
        self.config = system_config
        self.logger = industrial_logger.get_logger('ml_system')
        
        # Modelos especializados por tipo de problema
        self.leak_detector = None        # IsolationForest - detec√ß√£o de anomalias
        self.leak_classifier = None      # RandomForest - classifica√ß√£o de tipos
        self.status_detector = None      # Status operacional
        self.scaler = StandardScaler()   # Normaliza√ß√£o de features
        
        # Estado din√¢mico do modelo
        self.is_trained = False
        self.last_training_time = None
        self.training_data_buffer = []
        self.feature_names = []
        
        # Configura√ß√µes adaptativas baseadas em janela temporal
        self.feature_window = max(50, int(CONSTANTS.ML_TRAINING_WINDOW * 0.05))
        self.retrain_threshold = 100     # Retreino a cada 100 novos exemplos
        self.detection_threshold = -0.1  # Threshold adaptativo para anomalias
        
        # Sistema de cache para otimiza√ß√£o
        self.feature_cache = {}
```

#### üéØ Objetivos do Sistema

1. **Detec√ß√£o de Anomalias**: Identifica√ß√£o de padr√µes an√¥malos indicativos de vazamentos
2. **Classifica√ß√£o de Tipos**: Categoriza√ß√£o de diferentes tipos de problemas detectados
3. **Predi√ß√£o Temporal**: Previs√£o de tend√™ncias e comportamentos futuros
4. **Aprendizado Cont√≠nuo**: Adapta√ß√£o autom√°tica com novos dados confirmados

---

## üîÑ Arquitetura Adaptativa

### Pipeline de Processamento

#### üìä Fluxo de Dados

```mermaid
graph TD
    A[MultiVariableSnapshot] --> B[Feature Extraction]
    B --> C[Feature Scaling]
    C --> D[ML Algorithms]
    D --> E[Prediction Fusion]
    E --> F[Confidence Assessment]
    F --> G[Result Integration]
    
    H[Training Buffer] --> I[Adaptive Retraining]
    I --> D
    
    J[Cache System] --> B
    B --> J
```

#### ‚öôÔ∏è Configura√ß√£o Adaptativa

```python
def _configure_adaptive_parameters(self):
    """
    Configura par√¢metros adaptativos baseados no hist√≥rico do sistema
    
    Ajusta dinamicamente:
    - Janela de features baseada na variabilidade dos dados
    - Threshold de detec√ß√£o baseado na taxa de falsos positivos
    - Frequ√™ncia de retreino baseada na deriva de conceito
    """
    
    # Janela adaptativa baseada na estabilidade dos dados
    if len(self.training_data_buffer) > 100:
        recent_data = self.training_data_buffer[-100:]
        stability_metric = self._calculate_data_stability(recent_data)
        
        # Janela menor para dados inst√°veis (mais reativo)
        if stability_metric < 0.5:
            self.feature_window = max(25, int(self.feature_window * 0.8))
        else:
            self.feature_window = min(100, int(self.feature_window * 1.1))
    
    # Threshold adaptativo baseado na performance
    if hasattr(self, 'recent_predictions'):
        false_positive_rate = self._calculate_false_positive_rate()
        
        if false_positive_rate > 0.1:  # Muitos falsos positivos
            self.detection_threshold -= 0.02  # Mais conservador
        elif false_positive_rate < 0.05:  # Poucos falsos positivos
            self.detection_threshold += 0.01  # Mais sens√≠vel
```

### Sistema de Cache Inteligente

#### üíæ Cache de Features

```python
class FeatureCache:
    """Cache inteligente para features computacionalmente caras"""
    
    def __init__(self, max_size=1000):
        self.cache = {}
        self.access_times = {}
        self.computation_costs = {}
        self.max_size = max_size
        
    def get_cache_key(self, snapshots):
        """Gera chave √∫nica baseada no hash dos dados"""
        data_arrays = [
            np.array([s.expeditor_pressure for s in snapshots]),
            np.array([s.receiver_pressure for s in snapshots]),
            np.array([s.flow for s in snapshots]),
            np.array([s.density for s in snapshots]),
            np.array([s.temperature for s in snapshots])
        ]
        
        combined_hash = hashlib.md5()
        for arr in data_arrays:
            combined_hash.update(arr.tobytes())
        
        return combined_hash.hexdigest()[:16]
    
    def get_or_compute(self, key, compute_func, *args, **kwargs):
        """Recupera do cache ou computa se necess√°rio"""
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        
        # Computa e armazena
        start_time = time.time()
        result = compute_func(*args, **kwargs)
        computation_time = time.time() - start_time
        
        self._store_with_lru(key, result, computation_time)
        return result
```

---

## üîç Extra√ß√£o de Features Avan√ßadas

### M√©todo Principal: extract_advanced_features

Este √© o cora√ß√£o do sistema de ML, respons√°vel por extrair **81 features especializadas** de cada conjunto de snapshots.

#### üìà Vis√£o Geral das Features

```python
def extract_advanced_features(self, snapshots: List[MultiVariableSnapshot]) -> np.ndarray:
    """
    Extrai 81 features avan√ßadas especializadas em an√°lise hidr√°ulica industrial
    
    Categorias de Features:
    1. Estat√≠sticas (45 features) - Momentos, percentis, tend√™ncias
    2. Gradientes (9 features) - Derivadas e varia√ß√µes
    3. Correla√ß√£o Cruzada (5 features) - An√°lise s√¥nica avan√ßada
    4. Espectrais (8 features) - An√°lise de frequ√™ncias
    5. Relacionais (8 features) - Correla√ß√µes entre vari√°veis
    6. Temporais (6 features) - Estabilidade e persist√™ncia
    
    Total: 81 features altamente especializadas
    """
    
    if len(snapshots) < self.feature_window:
        raise ValueError(f"Necess√°rios pelo menos {self.feature_window} snapshots")
    
    # Extra√ß√£o dos sinais base
    exp_pressure = np.array([s.expeditor_pressure for s in snapshots])
    rec_pressure = np.array([s.receiver_pressure for s in snapshots])
    flow = np.array([s.flow for s in snapshots])
    density = np.array([s.density for s in snapshots])
    temperature = np.array([s.temperature for s in snapshots])
    
    features = []
    feature_names = []
    
    # Processa cada categoria sequencialmente
    self._extract_statistical_features(features, feature_names, exp_pressure, rec_pressure, flow, density, temperature)
    self._extract_gradient_features(features, feature_names, exp_pressure, rec_pressure, flow)
    self._extract_correlation_features(features, feature_names, exp_pressure, rec_pressure)
    self._extract_spectral_features(features, feature_names, exp_pressure, rec_pressure)
    self._extract_relational_features(features, feature_names, exp_pressure, rec_pressure, flow, density, temperature)
    self._extract_temporal_features(features, feature_names, exp_pressure, rec_pressure, flow)
    
    self.feature_names = feature_names
    return np.array(features)
```

---

## üìä Features Estat√≠sticas

### Estat√≠sticas Descritivas Completas

Para cada uma das 5 vari√°veis principais (press√£o expedidor, press√£o recebedor, fluxo, densidade, temperatura), o sistema extrai **9 features estat√≠sticas**:

#### üéØ Features Por Vari√°vel

```python
def _extract_statistical_features(self, features, feature_names, *signals):
    """
    Extrai features estat√≠sticas robustas para cada sinal
    
    Para cada sinal:
    1. M√©dia aritm√©tica
    2. Desvio padr√£o
    3. Valor m√≠nimo
    4. Valor m√°ximo  
    5. Quartil 25%
    6. Quartil 75%
    7. Mediana
    8. Segundo momento central (vari√¢ncia)
    9. Tend√™ncia linear (coeficiente angular)
    """
    
    signal_names = ['exp_p', 'rec_p', 'flow', 'density', 'temp']
    
    for signal, name in zip(signals, signal_names):
        # Estat√≠sticas de localiza√ß√£o
        mean_val = np.mean(signal)
        median_val = np.median(signal)
        
        # Estat√≠sticas de dispers√£o
        std_val = np.std(signal)
        q25 = np.percentile(signal, 25)
        q75 = np.percentile(signal, 75)
        
        # Estat√≠sticas de extremos
        min_val = np.min(signal)
        max_val = np.max(signal)
        
        # Segundo momento (vari√¢ncia amostral)
        variance = np.var(signal)
        
        # An√°lise de tend√™ncia temporal
        time_indices = np.arange(len(signal))
        trend_coeff = np.polyfit(time_indices, signal, 1)[0] if len(signal) > 1 else 0.0
        
        features.extend([
            mean_val, std_val, min_val, max_val,
            q25, q75, median_val, variance, trend_coeff
        ])
        
        feature_names.extend([
            f'{name}_mean', f'{name}_std', f'{name}_min', f'{name}_max',
            f'{name}_q25', f'{name}_q75', f'{name}_median', f'{name}_var', f'{name}_trend'
        ])
```

#### üìê Interpreta√ß√£o Matem√°tica

##### **M√©dia Aritm√©tica**

```
Œº = (1/n) √ó Œ£(x·µ¢)
```

- **Significado**: Centro de massa da distribui√ß√£o
- **Aplica√ß√£o**: N√≠vel operacional m√©dio da vari√°vel

##### **Desvio Padr√£o**

```
œÉ = ‚àö[(1/n) √ó Œ£(x·µ¢ - Œº)¬≤]
```

- **Significado**: Dispers√£o t√≠pica em torno da m√©dia
- **Aplica√ß√£o**: Estabilidade operacional

##### **Quartis (Q25, Q75)**

```
Q25: 25% dos dados ‚â§ Q25
Q75: 75% dos dados ‚â§ Q75
IQR = Q75 - Q25
```

- **Significado**: Intervalos robustos da distribui√ß√£o
- **Aplica√ß√£o**: Detec√ß√£o de outliers e assimetria

##### **Tend√™ncia Linear**

```
Œ≤ = Œ£[(t·µ¢ - tÃÑ)(x·µ¢ - xÃÑ)] / Œ£[(t·µ¢ - tÃÑ)¬≤]
```

- **Significado**: Taxa de mudan√ßa temporal
- **Aplica√ß√£o**: Deriva operacional, tend√™ncias de degrada√ß√£o

### Robustez Estat√≠stica

#### üõ°Ô∏è Tratamento de Outliers

```python
def robust_statistics(self, signal, contamination=0.1):
    """
    Calcula estat√≠sticas robustas resistentes a outliers
    
    M√©todos:
    - M√©dia aparada (trimmed mean)
    - Desvio absoluto mediano (MAD)
    - Quartis robustos
    """
    
    # Remove porcentagem de extremos para m√©dia aparada
    trim_percent = contamination * 100
    trimmed_mean = stats.trim_mean(signal, trim_percent)
    
    # Desvio Absoluto Mediano (mais robusto que desvio padr√£o)
    median_val = np.median(signal)
    mad = np.median(np.abs(signal - median_val))
    robust_std = 1.4826 * mad  # Fator para equival√™ncia com desvio padr√£o
    
    # Quartis com m√©todo robusto
    q25_robust, q75_robust = np.percentile(signal, [25, 75], method='median_unbiased')
    
    return {
        'robust_mean': trimmed_mean,
        'robust_std': robust_std,
        'robust_q25': q25_robust,
        'robust_q75': q75_robust,
        'outlier_resistance': True
    }
```

---

## üìà Features de Gradiente

### An√°lise de Derivadas Temporais

As features de gradiente capturam **varia√ß√µes instant√¢neas** nos sinais, essenciais para detectar mudan√ßas abruptas caracter√≠sticas de vazamentos.

#### üî¨ Implementa√ß√£o Matem√°tica

```python
def _extract_gradient_features(self, features, feature_names, exp_pressure, rec_pressure, flow):
    """
    Extrai features baseadas em gradientes (derivadas num√©ricas)
    
    Para press√£o expedidor, press√£o recebedor e fluxo:
    1. M√©dia do gradiente (tend√™ncia geral)
    2. Desvio padr√£o do gradiente (variabilidade de mudan√ßas)
    3. M√°ximo gradiente absoluto (maior varia√ß√£o instant√¢nea)
    
    Total: 3 sinais √ó 3 features = 9 features
    """
    
    for signal, name in [(exp_pressure, 'exp_p'), (rec_pressure, 'rec_p'), (flow, 'flow')]:
        # Gradiente usando diferen√ßas centradas para maior precis√£o
        gradient = np.gradient(signal)
        
        # Features do gradiente
        grad_mean = np.mean(gradient)         # Deriva m√©dia
        grad_std = np.std(gradient)           # Variabilidade das mudan√ßas
        grad_max_abs = np.max(np.abs(gradient))  # Maior mudan√ßa instant√¢nea
        
        features.extend([grad_mean, grad_std, grad_max_abs])
        feature_names.extend([f'{name}_grad_mean', f'{name}_grad_std', f'{name}_grad_max'])
```

#### üìä Significado das Features de Gradiente

##### **M√©dia do Gradiente**

```
‚àáÃÑ = (1/n-1) √ó Œ£[‚àáx·µ¢] = (1/n-1) √ó Œ£[x·µ¢‚Çä‚ÇÅ - x·µ¢]
```

- **Interpreta√ß√£o**: Taxa m√©dia de mudan√ßa
- **Aplica√ß√£o**: Detecta tend√™ncias de crescimento/decrescimento
- **Vazamentos**: Gradiente negativo persistente na press√£o

##### **Desvio Padr√£o do Gradiente**

```
œÉ‚àá = ‚àö[(1/n-1) √ó Œ£(‚àáx·µ¢ - ‚àáÃÑ)¬≤]
```

- **Interpreta√ß√£o**: Variabilidade das mudan√ßas instant√¢neas
- **Aplica√ß√£o**: Detecta instabilidade operacional
- **Vazamentos**: Alta variabilidade indica transientes

##### **M√°ximo Gradiente Absoluto**

```
|‚àá|‚Çò‚Çê‚Çì = max{|x·µ¢‚Çä‚ÇÅ - x·µ¢| : i = 1,...,n-1}
```

- **Interpreta√ß√£o**: Maior mudan√ßa instant√¢nea observada
- **Aplica√ß√£o**: Detecta eventos s√∫bitos
- **Vazamentos**: Picos indicam momento de in√≠cio do vazamento

#### üéØ Algoritmo de Gradiente Otimizado

```python
def enhanced_gradient_calculation(self, signal):
    """
    C√°lculo de gradiente com m√∫ltiplas t√©cnicas para maior robustez
    
    Implementa:
    1. Diferen√ßas centradas (padr√£o)
    2. Diferen√ßas progressivas/regressivas nas bordas
    3. Suaviza√ß√£o opcional para reduzir ru√≠do
    4. Detec√ß√£o de descontinuidades
    """
    
    n = len(signal)
    gradient = np.zeros(n)
    
    # Diferen√ßas progressivas na primeira amostra
    gradient[0] = signal[1] - signal[0]
    
    # Diferen√ßas centradas no interior (mais precisas)
    for i in range(1, n-1):
        gradient[i] = (signal[i+1] - signal[i-1]) / 2.0
    
    # Diferen√ßas regressivas na √∫ltima amostra
    gradient[-1] = signal[-1] - signal[-2]
    
    # Detec√ß√£o de descontinuidades (poss√≠veis artefatos)
    discontinuities = np.abs(gradient) > 3 * np.std(gradient)
    
    # Suaviza√ß√£o opcional em regi√µes com descontinuidades
    if np.any(discontinuities):
        from scipy.ndimage import gaussian_filter1d
        gradient_smooth = gaussian_filter1d(gradient, sigma=1.0)
        
        # Mistura gradiente original com suavizado
        alpha = 0.7  # Peso do gradiente original
        gradient = alpha * gradient + (1 - alpha) * gradient_smooth
    
    return {
        'gradient': gradient,
        'discontinuities': discontinuities,
        'max_discontinuity': np.max(np.abs(gradient[discontinuities])) if np.any(discontinuities) else 0.0
    }
```

---

## üîó Features de Correla√ß√£o Cruzada

### An√°lise S√¥nica Avan√ßada Integrada ao ML

As features de correla√ß√£o cruzada integram a **an√°lise s√¥nica** ao sistema de machine learning, fornecendo informa√ß√µes sobre a propaga√ß√£o de ondas de press√£o.

#### ‚ö° Implementa√ß√£o com FFT

```python
def _extract_correlation_features(self, features, feature_names, exp_pressure, rec_pressure):
    """
    Extrai features avan√ßadas da correla√ß√£o cruzada entre sensores
    
    Features extra√≠das:
    1. Correla√ß√£o m√°xima normalizada
    2. Delay em amostras (tempo de tr√¢nsito)
    3. N√∫mero de picos de correla√ß√£o
    4. Desvio padr√£o da correla√ß√£o
    5. M√©dia das correla√ß√µes positivas
    
    Utiliza FFT para efici√™ncia computacional O(N log N)
    """
    
    from scipy import signal as sp_signal
    
    # Correla√ß√£o cruzada usando FFT (mais eficiente)
    correlation = sp_signal.correlate(exp_pressure, rec_pressure, mode='full', method='fft')
    
    # Normaliza√ß√£o pela energia dos sinais
    correlation_normalized = correlation / len(exp_pressure)
    
    # Encontra pico m√°ximo
    max_corr_idx = np.argmax(np.abs(correlation_normalized))
    max_correlation = correlation_normalized[max_corr_idx]
    
    # Delay em amostras (diferen√ßa do centro)
    delay_samples = max_corr_idx - len(correlation_normalized)//2
    
    # An√°lise de m√∫ltiplos picos (indica reflex√µes/ecos)
    correlation_abs = np.abs(correlation_normalized)
    threshold = 0.1 * np.max(correlation_abs)
    correlation_peaks, _ = sp_signal.find_peaks(correlation_abs, height=threshold)
    n_peaks = len(correlation_peaks)
    
    # Variabilidade da correla√ß√£o
    correlation_std = np.std(correlation_normalized)
    
    # M√©dia das correla√ß√µes positivas (estabilidade do sinal)
    positive_correlations = correlation_normalized[correlation_normalized > 0]
    correlation_pos_mean = np.mean(positive_correlations) if len(positive_correlations) > 0 else 0.0
    
    features.extend([
        max_correlation, delay_samples, n_peaks, correlation_std, correlation_pos_mean
    ])
    
    feature_names.extend([
        'corr_max_norm', 'delay_samples', 'corr_peaks', 'corr_std', 'corr_pos_mean'
    ])
```

#### üßÆ Matem√°tica da Correla√ß√£o Cruzada

##### **Correla√ß√£o Cruzada Normalizada**

```
R_xy[m] = (1/N) √ó Œ£(n=0 to N-1) x[n] √ó y[n+m] / ‚àö(œÉ_x √ó œÉ_y)
```

##### **Detec√ß√£o de Picos M√∫ltiplos**

```python
def analyze_correlation_peaks(self, correlation, min_prominence=0.1):
    """
    Analisa picos na fun√ß√£o de correla√ß√£o para detectar ecos e reflex√µes
    
    Picos m√∫ltiplos podem indicar:
    - Reflex√µes em jun√ß√µes ou v√°lvulas
    - M√∫ltiplos caminhos de propaga√ß√£o  
    - Interfer√™ncias construtivas/destrutivas
    """
    
    # Localiza picos com promin√™ncia m√≠nima
    peaks, properties = find_peaks(
        np.abs(correlation), 
        height=min_prominence * np.max(np.abs(correlation)),
        prominence=0.05 * np.max(np.abs(correlation)),
        distance=5  # Separa√ß√£o m√≠nima entre picos
    )
    
    # Analisa caracter√≠sticas dos picos
    peak_analysis = {
        'n_peaks': len(peaks),
        'peak_positions': peaks.tolist(),
        'peak_heights': correlation[peaks].tolist(),
        'peak_prominences': properties['prominences'].tolist(),
        'primary_peak_idx': peaks[np.argmax(properties['peak_heights'])] if len(peaks) > 0 else 0
    }
    
    # Classifica√ß√£o baseada no n√∫mero de picos
    if len(peaks) == 1:
        classification = 'single_path'  # Caminho direto limpo
    elif len(peaks) == 2:
        classification = 'dual_path'    # Poss√≠vel reflex√£o principal
    else:
        classification = 'multi_path'   # M√∫ltiplas reflex√µes/ecos
    
    peak_analysis['path_classification'] = classification
    
    return peak_analysis
```

##### **Interpreta√ß√£o para Detec√ß√£o de Vazamentos**

| Feature | Opera√ß√£o Normal | Vazamento Incipiente | Vazamento Severo |
|---------|----------------|---------------------|------------------|
| **Correla√ß√£o M√°xima** | 0.7 - 0.95 | 0.4 - 0.7 | 0.1 - 0.4 |
| **Delay Samples** | Est√°vel ¬±2 | Deriva ¬±5 | Inst√°vel ¬±10+ |
| **N√∫mero de Picos** | 1-2 | 2-3 | 3+ |
| **Correla√ß√£o Std** | 0.05 - 0.15 | 0.15 - 0.3 | 0.3+ |

---

**AVISO: Este manual √© muito extenso. Para n√£o exceder o limite de resposta, vou continuar com as pr√≥ximas se√ß√µes em mensagens separadas.**

**Pr√≥ximas se√ß√µes a serem criadas:**

- MANUAL_04_MACHINE_LEARNING_PARTE_II.md - Features espectrais, temporais, relacionais
- MANUAL_04_MACHINE_LEARNING_PARTE_III.md - Algoritmos (Isolation Forest, Random Forest, SVM, DBSCAN)
- MANUAL_04_MACHINE_LEARNING_PARTE_IV.md - An√°lise PCA, Correla√ß√£o Can√¥nica, Predi√ß√£o

Continuar com a Parte II?
