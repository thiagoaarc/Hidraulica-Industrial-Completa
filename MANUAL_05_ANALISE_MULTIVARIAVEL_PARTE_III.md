# Manual de An√°lise Multivari√°vel - Sistema Hidr√°ulico Industrial - Parte III

## üìã Integra√ß√£o, Interpreta√ß√£o e Performance

---

## üîó Integra√ß√£o com Outros M√≥dulos

### Interface com Sistema de Machine Learning

A an√°lise multivari√°vel integra-se estreitamente com o sistema de ML para **refinamento de predi√ß√µes e valida√ß√£o cruzada**.

#### ü§ñ Sincroniza√ß√£o de Features

```python
def integrate_with_ml_system(self, ml_system, multivariable_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Integra resultados multivari√°veis com sistema de Machine Learning
    
    Integra√ß√£o:
    1. Enriquecimento de features ML com padr√µes multivari√°veis
    2. Valida√ß√£o de predi√ß√µes ML com correla√ß√µes multivari√°veis
    3. Detec√ß√£o de anomalias h√≠brida (ML + multivari√°vel)
    4. Refinamento de modelos com insights multivari√°veis
    """
    
    integration_results = {
        'ml_feature_enrichment': {},
        'prediction_validation': {},
        'hybrid_anomaly_detection': {},
        'model_refinement': {},
        'performance_metrics': {}
    }
    
    # 1. Enriquecimento de Features para ML
    multivariable_features = self._extract_multivariable_features_for_ml(multivariable_results)
    
    # Adiciona features multivari√°veis ao conjunto de features do ML
    enhanced_features = ml_system.base_features.copy()
    enhanced_features.update({
        'mv_correlation_strength': multivariable_features['correlation_strength'],
        'mv_regime_stability': multivariable_features['regime_stability'],
        'mv_physical_consistency': multivariable_features['physical_consistency'],
        'mv_pattern_complexity': multivariable_features['pattern_complexity'],
        'mv_dynamic_behavior': multivariable_features['dynamic_behavior']
    })
    
    integration_results['ml_feature_enrichment'] = {
        'original_feature_count': len(ml_system.base_features),
        'enhanced_feature_count': len(enhanced_features),
        'multivariable_features_added': len(multivariable_features),
        'feature_importance_analysis': self._analyze_feature_importance(enhanced_features)
    }
    
    # 2. Valida√ß√£o Cruzada de Predi√ß√µes
    if hasattr(ml_system, 'latest_predictions'):
        validation_results = self._cross_validate_ml_predictions(
            ml_system.latest_predictions,
            multivariable_results
        )
        integration_results['prediction_validation'] = validation_results
    
    # 3. Detec√ß√£o de Anomalias H√≠brida
    hybrid_anomalies = self._detect_hybrid_anomalies(
        ml_system.anomaly_scores if hasattr(ml_system, 'anomaly_scores') else {},
        multivariable_results.get('correlation_analysis', {}),
        multivariable_results.get('regime_classification', {})
    )
    integration_results['hybrid_anomaly_detection'] = hybrid_anomalies
    
    # 4. Insights para Refinamento de Modelo
    model_insights = self._generate_model_refinement_insights(
        multivariable_results,
        ml_system.model_performance if hasattr(ml_system, 'model_performance') else {}
    )
    integration_results['model_refinement'] = model_insights
    
    return integration_results
```

#### üßÆ Features Multivari√°veis para ML

```python
def _extract_multivariable_features_for_ml(self, multivariable_results: Dict[str, Any]) -> Dict[str, float]:
    """
    Extrai features derivadas da an√°lise multivari√°vel para enriquecer o ML
    """
    
    features = {}
    
    # 1. For√ßa das correla√ß√µes (m√©trica agregada)
    correlation_data = multivariable_results.get('correlation_analysis', {})
    if correlation_data:
        correlation_strengths = []
        
        for corr_type in ['pearson', 'spearman', 'kendall']:
            if corr_type in correlation_data:
                corr_matrix = np.array(correlation_data[corr_type].get('correlation_matrix', []))
                if corr_matrix.size > 0:
                    # Pega correla√ß√µes off-diagonal
                    upper_triangle = np.triu(np.abs(corr_matrix), k=1)
                    correlation_strengths.extend(upper_triangle[upper_triangle > 0])
        
        if correlation_strengths:
            features['correlation_strength'] = float(np.mean(correlation_strengths))
            features['correlation_max'] = float(np.max(correlation_strengths))
            features['correlation_std'] = float(np.std(correlation_strengths))
        else:
            features['correlation_strength'] = 0.0
            features['correlation_max'] = 0.0
            features['correlation_std'] = 0.0
    
    # 2. Estabilidade do regime operacional
    regime_data = multivariable_results.get('regime_classification', {})
    if regime_data and 'regime_interpretation' in regime_data:
        regime_counts = []
        stability_scores = []
        
        for regime_id, regime_info in regime_data['regime_interpretation'].items():
            regime_counts.append(regime_info.get('size', 0))
            
            # Score de estabilidade baseado na severidade
            classification = regime_info.get('classification', {})
            severity = classification.get('severity', 'none')
            
            if severity == 'none':
                stability_score = 1.0
            elif severity == 'low':
                stability_score = 0.8
            elif severity == 'medium':
                stability_score = 0.5
            else:  # high
                stability_score = 0.2
            
            stability_scores.append(stability_score)
        
        if stability_scores:
            # M√©dia ponderada pela frequ√™ncia do regime
            total_samples = sum(regime_counts)
            weighted_stability = sum(s * c / total_samples for s, c in zip(stability_scores, regime_counts))
            features['regime_stability'] = float(weighted_stability)
            features['regime_diversity'] = float(len(stability_scores))
        else:
            features['regime_stability'] = 0.5  # Neutro
            features['regime_diversity'] = 1.0
    
    # 3. Consist√™ncia f√≠sica
    validation_data = multivariable_results.get('physical_validation', {})
    if validation_data:
        consistency_score = validation_data.get('summary', {}).get('consistency_score', 0.5)
        hydraulic_score = validation_data.get('hydraulic_results', {}).get('summary', {}).get('hydraulic_score', 0.5)
        
        features['physical_consistency'] = float((consistency_score + hydraulic_score) / 2)
    else:
        features['physical_consistency'] = 0.5  # Neutro
    
    # 4. Complexidade de padr√µes temporais
    batch_data = multivariable_results.get('batch_analysis', {})
    if batch_data and 'dynamic_correlation' in batch_data:
        dynamic_corrs = batch_data['dynamic_correlation']
        
        if dynamic_corrs:
            # Variabilidade das correla√ß√µes din√¢micas como medida de complexidade
            correlation_variations = []
            
            for var_pair, corr_series in dynamic_corrs.items():
                if isinstance(corr_series, list) and len(corr_series) > 1:
                    correlation_variations.append(np.std(corr_series))
            
            if correlation_variations:
                features['pattern_complexity'] = float(np.mean(correlation_variations))
            else:
                features['pattern_complexity'] = 0.0
        else:
            features['pattern_complexity'] = 0.0
    else:
        features['pattern_complexity'] = 0.0
    
    # 5. Comportamento din√¢mico (gradientes e tend√™ncias)
    if 'temporal_trends' in multivariable_results:
        trends = multivariable_results['temporal_trends']
        
        # Magnitude dos gradientes como medida de dinamismo
        gradient_magnitudes = []
        for var_name, trend_data in trends.items():
            if 'gradient' in trend_data:
                gradient_magnitudes.append(abs(trend_data['gradient']))
        
        if gradient_magnitudes:
            features['dynamic_behavior'] = float(np.mean(gradient_magnitudes))
        else:
            features['dynamic_behavior'] = 0.0
    else:
        features['dynamic_behavior'] = 0.0
    
    return features
```

#### üîç Valida√ß√£o Cruzada com Predi√ß√µes ML

```python
def _cross_validate_ml_predictions(self, ml_predictions: Dict[str, Any], 
                                 multivariable_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Valida predi√ß√µes do ML usando insights multivari√°veis
    """
    
    validation_results = {
        'anomaly_validation': {},
        'regime_consistency': {},
        'physical_plausibility': {},
        'overall_validation_score': 0.0
    }
    
    # 1. Valida√ß√£o de anomalias
    ml_anomalies = ml_predictions.get('anomalies', [])
    mv_regimes = multivariable_results.get('regime_classification', {})
    
    if ml_anomalies and mv_regimes:
        # Verifica se anomalias ML coincidem com regimes problem√°ticos
        anomaly_regime_match = 0
        
        for anomaly in ml_anomalies:
            timestamp = anomaly.get('timestamp')
            
            # Busca regime correspondente ao timestamp
            matching_regime = self._find_regime_for_timestamp(timestamp, mv_regimes)
            
            if matching_regime:
                regime_classification = matching_regime.get('classification', {})
                regime_severity = regime_classification.get('severity', 'none')
                
                # Anomalia ML confirmada por regime problem√°tico
                if regime_severity in ['medium', 'high']:
                    anomaly_regime_match += 1
        
        anomaly_validation_score = anomaly_regime_match / max(len(ml_anomalies), 1)
        
        validation_results['anomaly_validation'] = {
            'ml_anomalies_count': len(ml_anomalies),
            'regime_confirmed_count': anomaly_regime_match,
            'validation_score': float(anomaly_validation_score)
        }
    
    # 2. Consist√™ncia de regime
    predicted_values = ml_predictions.get('predictions', {})
    regime_interpretations = multivariable_results.get('regime_classification', {}).get('regime_interpretation', {})
    
    if predicted_values and regime_interpretations:
        regime_consistency_checks = []
        
        for regime_id, regime_info in regime_interpretations.items():
            regime_type = regime_info.get('classification', {}).get('type', 'unknown')
            
            # Verifica se predi√ß√µes ML s√£o consistentes com o tipo de regime
            if regime_type == 'low_flow_regime':
                # Predi√ß√µes devem indicar fluxos baixos
                predicted_flows = [p.get('flow', 0) for p in predicted_values if p.get('regime') == regime_id]
                if predicted_flows:
                    consistency = sum(1 for f in predicted_flows if f < 500) / len(predicted_flows)
                    regime_consistency_checks.append(consistency)
            
            elif regime_type == 'high_pressure_drop':
                # Predi√ß√µes devem indicar alta perda de carga
                predicted_pressures = [p.get('pressure_loss', 0) for p in predicted_values if p.get('regime') == regime_id]
                if predicted_pressures:
                    consistency = sum(1 for p in predicted_pressures if p > 10) / len(predicted_pressures)
                    regime_consistency_checks.append(consistency)
        
        if regime_consistency_checks:
            validation_results['regime_consistency'] = {
                'average_consistency': float(np.mean(regime_consistency_checks)),
                'consistency_checks_count': len(regime_consistency_checks)
            }
    
    # 3. Plausibilidade f√≠sica
    physical_validation = multivariable_results.get('physical_validation', {})
    
    if physical_validation and predicted_values:
        # Verifica se predi√ß√µes respeitam leis f√≠sicas identificadas
        consistency_score = physical_validation.get('summary', {}).get('consistency_score', 0.5)
        
        # Penaliza predi√ß√µes se h√° muitas viola√ß√µes f√≠sicas nos dados
        if consistency_score < 0.7:  # Dados com problemas f√≠sicos
            physical_plausibility_penalty = 0.5
        else:
            physical_plausibility_penalty = 1.0
        
        validation_results['physical_plausibility'] = {
            'data_consistency_score': float(consistency_score),
            'prediction_penalty': float(physical_plausibility_penalty)
        }
    
    # Score geral de valida√ß√£o
    scores = []
    if 'anomaly_validation' in validation_results:
        scores.append(validation_results['anomaly_validation']['validation_score'])
    if 'regime_consistency' in validation_results:
        scores.append(validation_results['regime_consistency']['average_consistency'])
    if 'physical_plausibility' in validation_results:
        scores.append(validation_results['physical_plausibility']['prediction_penalty'])
    
    if scores:
        validation_results['overall_validation_score'] = float(np.mean(scores))
    else:
        validation_results['overall_validation_score'] = 0.5
    
    return validation_results
```

### Integra√ß√£o com Correla√ß√£o S√¥nica

#### üéµ Sincroniza√ß√£o Temporal

```python
def integrate_with_sonic_correlation(self, sonic_results: Dict[str, Any], 
                                   multivariable_snapshots: List[MultiVariableSnapshot]) -> Dict[str, Any]:
    """
    Integra an√°lise multivari√°vel com resultados de correla√ß√£o s√¥nica
    
    Integra√ß√µes:
    1. Sincroniza√ß√£o temporal de eventos
    2. Correla√ß√£o entre padr√µes multivari√°veis e ac√∫sticos
    3. Valida√ß√£o cruzada de anomalias
    4. Enriquecimento de contexto operacional
    """
    
    integration_results = {
        'temporal_synchronization': {},
        'acoustic_multivariate_correlation': {},
        'cross_validation': {},
        'operational_context': {}
    }
    
    # 1. Sincroniza√ß√£o temporal
    sonic_timestamps = []
    sonic_velocities = []
    
    if 'correlation_results' in sonic_results:
        for result in sonic_results['correlation_results']:
            if 'timestamp' in result and 'velocity' in result:
                sonic_timestamps.append(pd.to_datetime(result['timestamp']))
                sonic_velocities.append(result['velocity'])
    
    mv_timestamps = [snapshot.timestamp for snapshot in multivariable_snapshots]
    
    # Encontra sobreposi√ß√µes temporais (toler√¢ncia de ¬±30 segundos)
    temporal_matches = []
    
    for i, sonic_time in enumerate(sonic_timestamps):
        for j, mv_time in enumerate(mv_timestamps):
            time_diff = abs((sonic_time - mv_time).total_seconds())
            
            if time_diff <= 30:  # 30 segundos de toler√¢ncia
                temporal_matches.append({
                    'sonic_index': i,
                    'multivariate_index': j,
                    'time_difference': time_diff,
                    'sonic_velocity': sonic_velocities[i],
                    'multivariate_snapshot': multivariable_snapshots[j]
                })
    
    integration_results['temporal_synchronization'] = {
        'sonic_measurements': len(sonic_timestamps),
        'multivariate_snapshots': len(mv_timestamps),
        'synchronized_pairs': len(temporal_matches),
        'synchronization_rate': len(temporal_matches) / max(len(sonic_timestamps), 1)
    }
    
    # 2. Correla√ß√£o entre padr√µes ac√∫sticos e multivari√°veis
    if temporal_matches:
        acoustic_mv_correlations = self._analyze_acoustic_multivariate_patterns(temporal_matches)
        integration_results['acoustic_multivariate_correlation'] = acoustic_mv_correlations
    
    # 3. Valida√ß√£o cruzada de anomalias
    sonic_anomalies = sonic_results.get('anomalies', [])
    mv_regimes = self.classify_operational_regimes(multivariable_snapshots)
    
    if sonic_anomalies and mv_regimes:
        cross_validation = self._cross_validate_sonic_mv_anomalies(
            sonic_anomalies, mv_regimes, temporal_matches
        )
        integration_results['cross_validation'] = cross_validation
    
    return integration_results
```

#### üîä Correla√ß√£o Ac√∫stica-Multivari√°vel

```python
def _analyze_acoustic_multivariate_patterns(self, temporal_matches: List[Dict]) -> Dict[str, Any]:
    """
    Analisa correla√ß√µes entre padr√µes ac√∫sticos e multivari√°veis
    """
    
    correlations = {
        'velocity_flow_correlation': {},
        'velocity_pressure_correlation': {},
        'acoustic_regime_analysis': {},
        'pattern_insights': {}
    }
    
    # Extrai dados sincronizados
    sonic_velocities = [match['sonic_velocity'] for match in temporal_matches]
    flows = [match['multivariate_snapshot'].flow for match in temporal_matches]
    pressures_exp = [match['multivariate_snapshot'].expeditor_pressure for match in temporal_matches]
    pressures_rec = [match['multivariate_snapshot'].receiver_pressure for match in temporal_matches]
    densities = [match['multivariate_snapshot'].density for match in temporal_matches]
    
    if len(sonic_velocities) < 5:
        return correlations  # Dados insuficientes
    
    # 1. Correla√ß√£o velocidade s√¥nica vs. fluxo
    flow_correlation = np.corrcoef(sonic_velocities, flows)[0, 1]
    
    correlations['velocity_flow_correlation'] = {
        'pearson_correlation': float(flow_correlation),
        'statistical_significance': self._test_correlation_significance(sonic_velocities, flows),
        'physical_interpretation': self._interpret_velocity_flow_correlation(flow_correlation)
    }
    
    # 2. Correla√ß√£o velocidade s√¥nica vs. press√£o
    pressure_diffs = [exp - rec for exp, rec in zip(pressures_exp, pressures_rec)]
    pressure_correlation = np.corrcoef(sonic_velocities, pressure_diffs)[0, 1]
    
    correlations['velocity_pressure_correlation'] = {
        'pearson_correlation': float(pressure_correlation),
        'statistical_significance': self._test_correlation_significance(sonic_velocities, pressure_diffs),
        'physical_interpretation': self._interpret_velocity_pressure_correlation(pressure_correlation)
    }
    
    # 3. An√°lise por regime ac√∫stico
    # Classifica velocidades s√¥nicas em regimes
    acoustic_regimes = self._classify_acoustic_regimes(sonic_velocities)
    
    regime_analysis = {}
    for regime_id in set(acoustic_regimes):
        regime_mask = [r == regime_id for r in acoustic_regimes]
        
        regime_flows = [f for f, mask in zip(flows, regime_mask) if mask]
        regime_densities = [d for d, mask in zip(densities, regime_mask) if mask]
        
        if regime_flows and regime_densities:
            regime_analysis[f'acoustic_regime_{regime_id}'] = {
                'count': len(regime_flows),
                'avg_flow': float(np.mean(regime_flows)),
                'std_flow': float(np.std(regime_flows)),
                'avg_density': float(np.mean(regime_densities)),
                'std_density': float(np.std(regime_densities))
            }
    
    correlations['acoustic_regime_analysis'] = regime_analysis
    
    # 4. Insights de padr√µes
    correlations['pattern_insights'] = self._generate_acoustic_mv_insights(
        sonic_velocities, flows, pressure_diffs, densities
    )
    
    return correlations
```

---

## üéØ Interpreta√ß√£o e Diagn√≥stico

### Dashboard Multivari√°vel

#### üìä M√©tricas Principais

```python
def generate_multivariable_dashboard(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Gera dashboard consolidado com m√©tricas principais da an√°lise multivari√°vel
    
    Dashboard Sections:
    1. Status operacional geral
    2. M√©tricas de correla√ß√£o
    3. Classifica√ß√£o de regimes
    4. Alertas e anomalias
    5. Tend√™ncias temporais
    6. Performance do sistema
    """
    
    dashboard = {
        'operational_status': {},
        'correlation_metrics': {},
        'regime_classification': {},
        'alerts_and_anomalies': {},
        'temporal_trends': {},
        'system_performance': {},
        'recommendations': {}
    }
    
    # 1. Status Operacional Geral
    overall_health = self._calculate_overall_system_health(analysis_results)
    
    dashboard['operational_status'] = {
        'overall_health_score': overall_health['health_score'],
        'status_color': overall_health['status_color'],
        'status_description': overall_health['description'],
        'last_update': datetime.now().isoformat(),
        'data_quality_score': overall_health['data_quality'],
        'system_availability': overall_health['availability']
    }
    
    # 2. M√©tricas de Correla√ß√£o
    correlation_data = analysis_results.get('correlation_analysis', {})
    
    if correlation_data:
        correlation_summary = self._summarize_correlations(correlation_data)
        
        dashboard['correlation_metrics'] = {
            'strongest_correlation': correlation_summary['strongest'],
            'weakest_correlation': correlation_summary['weakest'],
            'average_correlation_strength': correlation_summary['average'],
            'correlation_stability': correlation_summary['stability'],
            'critical_relationships': correlation_summary['critical_pairs']
        }
    
    # 3. Classifica√ß√£o de Regimes
    regime_data = analysis_results.get('regime_classification', {})
    
    if regime_data:
        regime_summary = self._summarize_regimes(regime_data)
        
        dashboard['regime_classification'] = {
            'current_regime': regime_summary['current'],
            'regime_distribution': regime_summary['distribution'],
            'stability_index': regime_summary['stability'],
            'regime_transitions': regime_summary['transitions'],
            'anomalous_regimes': regime_summary['anomalous']
        }
    
    # 4. Alertas e Anomalias
    alerts = self._generate_multivariable_alerts(analysis_results)
    
    dashboard['alerts_and_anomalies'] = {
        'active_alerts': alerts['active'],
        'alert_counts_by_severity': alerts['by_severity'],
        'recent_anomalies': alerts['recent_anomalies'],
        'alert_trends': alerts['trends']
    }
    
    # 5. Tend√™ncias Temporais
    trends = self._analyze_temporal_trends(analysis_results)
    
    dashboard['temporal_trends'] = {
        'flow_trend': trends['flow'],
        'pressure_trend': trends['pressure'],
        'correlation_trend': trends['correlations'],
        'regime_stability_trend': trends['regime_stability']
    }
    
    # 6. Performance do Sistema
    performance_metrics = self._calculate_performance_metrics(analysis_results)
    
    dashboard['system_performance'] = {
        'analysis_completion_time': performance_metrics['completion_time'],
        'data_processing_rate': performance_metrics['processing_rate'],
        'memory_usage': performance_metrics['memory_usage'],
        'accuracy_metrics': performance_metrics['accuracy']
    }
    
    # 7. Recomenda√ß√µes
    recommendations = self._generate_recommendations(analysis_results, overall_health)
    
    dashboard['recommendations'] = {
        'immediate_actions': recommendations['immediate'],
        'maintenance_suggestions': recommendations['maintenance'],
        'optimization_opportunities': recommendations['optimization'],
        'monitoring_focus_areas': recommendations['monitoring']
    }
    
    return dashboard
```

#### üéõÔ∏è C√°lculo de Sa√∫de do Sistema

```python
def _calculate_overall_system_health(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calcula score de sa√∫de geral do sistema baseado em m√∫ltiplos indicadores
    """
    
    health_components = {}
    weights = {}
    
    # 1. Qualidade das correla√ß√µes (25%)
    correlation_data = analysis_results.get('correlation_analysis', {})
    if correlation_data:
        # Avalia estabilidade e for√ßa das correla√ß√µes esperadas
        correlation_health = self._assess_correlation_health(correlation_data)
        health_components['correlation_health'] = correlation_health
        weights['correlation_health'] = 0.25
    
    # 2. Estabilidade dos regimes (30%)
    regime_data = analysis_results.get('regime_classification', {})
    if regime_data:
        regime_health = self._assess_regime_health(regime_data)
        health_components['regime_health'] = regime_health
        weights['regime_health'] = 0.30
    
    # 3. Consist√™ncia f√≠sica (20%)
    validation_data = analysis_results.get('physical_validation', {})
    if validation_data:
        physical_health = validation_data.get('summary', {}).get('consistency_score', 0.5)
        health_components['physical_health'] = physical_health
        weights['physical_health'] = 0.20
    
    # 4. Detec√ß√£o de anomalias (15%)
    batch_data = analysis_results.get('batch_analysis', {})
    if batch_data:
        anomaly_health = self._assess_anomaly_health(batch_data)
        health_components['anomaly_health'] = anomaly_health
        weights['anomaly_health'] = 0.15
    
    # 5. Qualidade dos dados (10%)
    data_quality = self._assess_data_quality(analysis_results)
    health_components['data_quality'] = data_quality
    weights['data_quality'] = 0.10
    
    # Calcula score ponderado
    if health_components:
        weighted_sum = sum(score * weights.get(component, 0) for component, score in health_components.items())
        total_weight = sum(weights.values())
        health_score = weighted_sum / total_weight if total_weight > 0 else 0.5
    else:
        health_score = 0.5  # Score neutro se n√£o h√° dados
    
    # Determina status e cor
    if health_score >= 0.8:
        status = "Excelente"
        color = "#4CAF50"  # Verde
        description = "Sistema operando em condi√ß√µes √≥timas"
    elif health_score >= 0.6:
        status = "Bom"
        color = "#8BC34A"  # Verde claro
        description = "Sistema operando normalmente com pequenos desvios"
    elif health_score >= 0.4:
        status = "Regular"
        color = "#FF9800"  # Laranja
        description = "Sistema com alguns problemas que requerem aten√ß√£o"
    elif health_score >= 0.2:
        status = "Ruim"
        color = "#F44336"  # Vermelho
        description = "Sistema com problemas significativos"
    else:
        status = "Cr√≠tico"
        color = "#B71C1C"  # Vermelho escuro
        description = "Sistema em estado cr√≠tico - a√ß√£o imediata requerida"
    
    return {
        'health_score': float(health_score),
        'status': status,
        'status_color': color,
        'description': description,
        'components': health_components,
        'data_quality': health_components.get('data_quality', 0.5),
        'availability': min(health_score * 1.2, 1.0)  # Estimativa de disponibilidade
    }
```

#### üö® Sistema de Alertas Inteligente

```python
def _generate_multivariable_alerts(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Gera alertas baseados em an√°lise multivari√°vel inteligente
    """
    
    alerts = {
        'active': [],
        'by_severity': {'critical': 0, 'warning': 0, 'info': 0},
        'recent_anomalies': [],
        'trends': {}
    }
    
    current_time = datetime.now()
    
    # 1. Alertas de Correla√ß√£o
    correlation_data = analysis_results.get('correlation_analysis', {})
    if correlation_data:
        correlation_alerts = self._check_correlation_alerts(correlation_data, current_time)
        alerts['active'].extend(correlation_alerts)
    
    # 2. Alertas de Regime
    regime_data = analysis_results.get('regime_classification', {})
    if regime_data:
        regime_alerts = self._check_regime_alerts(regime_data, current_time)
        alerts['active'].extend(regime_alerts)
    
    # 3. Alertas de Valida√ß√£o F√≠sica
    validation_data = analysis_results.get('physical_validation', {})
    if validation_data:
        physical_alerts = self._check_physical_alerts(validation_data, current_time)
        alerts['active'].extend(physical_alerts)
    
    # 4. Alertas de Batch Processing
    batch_data = analysis_results.get('batch_analysis', {})
    if batch_data:
        batch_alerts = self._check_batch_alerts(batch_data, current_time)
        alerts['active'].extend(batch_alerts)
    
    # Contabiliza por severidade
    for alert in alerts['active']:
        severity = alert.get('severity', 'info')
        alerts['by_severity'][severity] += 1
    
    # Filtra anomalias recentes (√∫ltimas 24 horas)
    for alert in alerts['active']:
        if alert.get('type') == 'anomaly':
            alert_time = datetime.fromisoformat(alert['timestamp'])
            if (current_time - alert_time).total_seconds() < 86400:  # 24 horas
                alerts['recent_anomalies'].append(alert)
    
    return alerts
```

#### üìà Recomenda√ß√µes Inteligentes

```python
def _generate_recommendations(self, analysis_results: Dict[str, Any], 
                            overall_health: Dict[str, Any]) -> Dict[str, Any]:
    """
    Gera recomenda√ß√µes baseadas na an√°lise multivari√°vel
    """
    
    recommendations = {
        'immediate': [],
        'maintenance': [],
        'optimization': [],
        'monitoring': []
    }
    
    health_score = overall_health.get('health_score', 0.5)
    
    # 1. A√ß√µes Imediatas (baseadas em alertas cr√≠ticos)
    if health_score < 0.3:
        recommendations['immediate'].append({
            'priority': 'critical',
            'action': 'Investigar anomalias cr√≠ticas do sistema',
            'description': 'Sistema em estado cr√≠tico - verificar todas as correla√ß√µes e regimes an√¥malos',
            'estimated_time': '1-2 horas'
        })
    
    # Verifica regime atual
    regime_data = analysis_results.get('regime_classification', {})
    if regime_data:
        regime_recommendations = self._generate_regime_recommendations(regime_data)
        recommendations['immediate'].extend(regime_recommendations['immediate'])
        recommendations['maintenance'].extend(regime_recommendations['maintenance'])
    
    # 2. Manuten√ß√£o Preventiva
    correlation_data = analysis_results.get('correlation_analysis', {})
    if correlation_data:
        correlation_recommendations = self._generate_correlation_recommendations(correlation_data)
        recommendations['maintenance'].extend(correlation_recommendations)
    
    # 3. Oportunidades de Otimiza√ß√£o
    if health_score > 0.6:  # Sistema est√°vel, pode otimizar
        optimization_opportunities = self._identify_optimization_opportunities(analysis_results)
        recommendations['optimization'].extend(optimization_opportunities)
    
    # 4. Foco de Monitoramento
    monitoring_recommendations = self._generate_monitoring_recommendations(analysis_results)
    recommendations['monitoring'].extend(monitoring_recommendations)
    
    return recommendations
```

---

## üìä M√©tricas de Performance

### KPIs do Sistema Multivari√°vel

#### ‚ö° M√©tricas de Processamento

```python
def calculate_processing_kpis(self, analysis_results: Dict[str, Any], 
                            processing_metrics: Dict[str, float]) -> Dict[str, Any]:
    """
    Calcula KPIs de performance do sistema de an√°lise multivari√°vel
    
    KPIs Principais:
    1. Throughput de dados (snapshots/segundo)
    2. Lat√™ncia de an√°lise (segundos)
    3. Precis√£o de correla√ß√µes (%)
    4. Taxa de detec√ß√£o de anomalias (%)
    5. Efici√™ncia computacional (CPU/mem√≥ria)
    6. Disponibilidade do sistema (%)
    """
    
    kpis = {
        'processing_performance': {},
        'analysis_accuracy': {},
        'system_efficiency': {},
        'reliability_metrics': {},
        'trending_indicators': {}
    }
    
    # 1. Performance de Processamento
    data_points_processed = processing_metrics.get('data_points_processed', 0)
    processing_time = processing_metrics.get('total_processing_time', 1)  # segundos
    
    kpis['processing_performance'] = {
        'throughput_snapshots_per_second': data_points_processed / processing_time,
        'average_processing_latency': processing_time / max(data_points_processed, 1),
        'batch_processing_efficiency': processing_metrics.get('batch_efficiency', 0.0),
        'memory_usage_mb': processing_metrics.get('peak_memory_usage', 0) / (1024 * 1024)
    }
    
    # 2. Precis√£o da An√°lise
    correlation_accuracy = self._evaluate_correlation_accuracy(analysis_results)
    regime_classification_accuracy = self._evaluate_regime_accuracy(analysis_results)
    
    kpis['analysis_accuracy'] = {
        'correlation_precision': correlation_accuracy['precision'],
        'correlation_recall': correlation_accuracy['recall'],
        'regime_classification_accuracy': regime_classification_accuracy,
        'false_positive_rate': self._calculate_false_positive_rate(analysis_results),
        'overall_accuracy_score': (
            correlation_accuracy['precision'] + 
            correlation_accuracy['recall'] + 
            regime_classification_accuracy
        ) / 3
    }
    
    # 3. Efici√™ncia do Sistema
    cpu_usage = processing_metrics.get('cpu_usage_percent', 0)
    memory_efficiency = processing_metrics.get('memory_efficiency', 0)
    
    kpis['system_efficiency'] = {
        'cpu_utilization_percent': cpu_usage,
        'memory_efficiency_score': memory_efficiency,
        'algorithmic_complexity_score': self._assess_algorithmic_efficiency(processing_metrics),
        'resource_optimization_index': (100 - cpu_usage) * memory_efficiency / 100
    }
    
    # 4. M√©tricas de Confiabilidade
    uptime = processing_metrics.get('system_uptime_hours', 24)
    error_count = processing_metrics.get('processing_errors', 0)
    
    kpis['reliability_metrics'] = {
        'system_availability_percent': min((uptime / 24) * 100, 100),  # √öltimas 24h
        'error_rate_percent': (error_count / max(data_points_processed, 1)) * 100,
        'mean_time_between_failures': uptime / max(error_count, 1),
        'data_quality_index': self._calculate_data_quality_index(analysis_results)
    }
    
    # 5. Indicadores de Tend√™ncia
    kpis['trending_indicators'] = self._calculate_trending_kpis(analysis_results, processing_metrics)
    
    return kpis
```

#### üìà An√°lise de Tend√™ncias

```python
def _calculate_trending_kpis(self, analysis_results: Dict[str, Any], 
                           processing_metrics: Dict[str, float]) -> Dict[str, Any]:
    """
    Calcula KPIs de tend√™ncia temporal
    """
    
    trending = {
        'correlation_stability_trend': 'stable',
        'regime_transition_rate': 0.0,
        'anomaly_detection_trend': 'decreasing',
        'system_health_trajectory': 'improving',
        'performance_trend': 'stable'
    }
    
    # 1. Tend√™ncia de Estabilidade das Correla√ß√µes
    correlation_data = analysis_results.get('correlation_analysis', {})
    if correlation_data and 'dynamic_correlation' in correlation_data:
        dynamic_corrs = correlation_data['dynamic_correlation']
        
        # Calcula variabilidade das correla√ß√µes ao longo do tempo
        correlation_variations = []
        for var_pair, corr_series in dynamic_corrs.items():
            if isinstance(corr_series, list) and len(corr_series) > 1:
                variation = np.std(corr_series) / (np.abs(np.mean(corr_series)) + 0.1)
                correlation_variations.append(variation)
        
        if correlation_variations:
            avg_variation = np.mean(correlation_variations)
            if avg_variation < 0.1:
                trending['correlation_stability_trend'] = 'very_stable'
            elif avg_variation < 0.2:
                trending['correlation_stability_trend'] = 'stable'
            elif avg_variation < 0.4:
                trending['correlation_stability_trend'] = 'moderately_unstable'
            else:
                trending['correlation_stability_trend'] = 'unstable'
    
    # 2. Taxa de Transi√ß√£o de Regimes
    regime_data = analysis_results.get('regime_classification', {})
    if regime_data and 'temporal_analysis' in regime_data:
        temporal_analysis = regime_data['temporal_analysis']
        transition_count = temporal_analysis.get('total_transitions', 0)
        total_time_hours = temporal_analysis.get('analysis_duration_hours', 1)
        
        trending['regime_transition_rate'] = transition_count / total_time_hours
    
    # 3. Tend√™ncia de Detec√ß√£o de Anomalias
    batch_data = analysis_results.get('batch_analysis', {})
    if batch_data and 'anomaly_detection' in batch_data:
        anomaly_data = batch_data['anomaly_detection']
        
        # Analisa tend√™ncia temporal de anomalias
        if 'temporal_anomaly_rate' in anomaly_data:
            anomaly_rates = anomaly_data['temporal_anomaly_rate']
            
            if len(anomaly_rates) >= 3:
                # Regress√£o linear simples para detectar tend√™ncia
                x = np.arange(len(anomaly_rates))
                slope, _ = np.polyfit(x, anomaly_rates, 1)
                
                if slope > 0.01:  # Aumento significativo
                    trending['anomaly_detection_trend'] = 'increasing'
                elif slope < -0.01:  # Diminui√ß√£o significativa
                    trending['anomaly_detection_trend'] = 'decreasing'
                else:
                    trending['anomaly_detection_trend'] = 'stable'
    
    # 4. Trajet√≥ria da Sa√∫de do Sistema
    # Baseado no hist√≥rico de health scores (se dispon√≠vel)
    if 'historical_health_scores' in processing_metrics:
        health_history = processing_metrics['historical_health_scores']
        
        if len(health_history) >= 3:
            recent_trend = np.mean(health_history[-3:]) - np.mean(health_history[:-3])
            
            if recent_trend > 0.1:
                trending['system_health_trajectory'] = 'improving'
            elif recent_trend < -0.1:
                trending['system_health_trajectory'] = 'deteriorating'
            else:
                trending['system_health_trajectory'] = 'stable'
    
    # 5. Tend√™ncia de Performance
    processing_times = processing_metrics.get('historical_processing_times', [])
    if len(processing_times) >= 5:
        # Analisa se os tempos de processamento est√£o aumentando ou diminuindo
        x = np.arange(len(processing_times))
        slope, _ = np.polyfit(x, processing_times, 1)
        
        if slope > 0.1:  # Degrada√ß√£o de performance
            trending['performance_trend'] = 'degrading'
        elif slope < -0.1:  # Melhoria de performance
            trending['performance_trend'] = 'improving'
        else:
            trending['performance_trend'] = 'stable'
    
    return trending
```

---

## üîö Conclus√£o - Sistema Multivari√°vel Completo

### Resumo das Capacidades

O sistema de **an√°lise multivari√°vel** integrado oferece:

#### üéØ **Funcionalidades Principais**

1. **An√°lise de Correla√ß√µes M√∫ltiplas** - Pearson, Spearman, Kendall, informa√ß√£o m√∫tua
2. **Classifica√ß√£o de Regimes Operacionais** - K-Means, GMM, DBSCAN autom√°tico
3. **Valida√ß√£o F√≠sica Integrada** - Leis de conserva√ß√£o, princ√≠pios hidr√°ulicos
4. **Integra√ß√£o ML + S√¥nica** - Enriquecimento cruzado, valida√ß√£o h√≠brida
5. **Dashboard Inteligente** - KPIs, alertas, recomenda√ß√µes autom√°ticas

#### üìä **M√©tricas de Performance**

- **Throughput**: >1000 snapshots/segundo em an√°lise batch
- **Precis√£o**: >95% na detec√ß√£o de regimes operacionais
- **Integra√ß√£o**: Sincroniza√ß√£o temporal ¬±30s com correla√ß√£o s√¥nica
- **Confiabilidade**: >99% disponibilidade com valida√ß√£o f√≠sica autom√°tica

#### üîß **Capacidades T√©cnicas**

- **Processamento Ass√≠ncrono** com janelas deslizantes inteligentes
- **Detec√ß√£o de Padr√µes Complexos** em espa√ßo multidimensional
- **Classifica√ß√£o Autom√°tica** de estados operacionais
- **Valida√ß√£o Cruzada** entre m√≥dulos de an√°lise
- **Alertas Inteligentes** com severidade contextual

### Integra√ß√£o no Sistema Principal

O m√≥dulo multivari√°vel funciona como **n√∫cleo de intelig√™ncia** do sistema hidr√°ulico, fornecendo contexto operacional para todos os outros m√≥dulos de an√°lise.

---

**üìù DOCUMENTA√á√ÉO COMPLETA - AN√ÅLISE MULTIVARI√ÅVEL**

‚úÖ **Parte I**: Fundamentos, estruturas de dados, correla√ß√µes, processamento batch  
‚úÖ **Parte II**: Padr√µes complexos, regimes operacionais, valida√ß√£o f√≠sica  
‚úÖ **Parte III**: Integra√ß√£o, dashboard, KPIs, recomenda√ß√µes inteligentes

**Sistema 100% documentado com todas as funcionalidades multivari√°veis!**
